{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Pre-entrenamiento "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se implementarán diversos AE con el fin de pre-entrenar redes profundas. De esta forma, se espera poder regularizar un modelo determinado, buscando evitar que este sufra de overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importación de módulos necesarios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce 820M (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.0 Creación de conjuntos de datos a utilizar**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análogamente a las secciones 3.1 y 3.2, se construyen los conjuntos de entrenamiento, validación y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se cargan conjutos de entrenamiento y de prueba\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Se normalizan conjuntos de datos en base a intensidad máxima de pixel\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# Se transforman las imágenes de ambos conjuntos a vectores\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "# Se define conjunto de validación y se reestructura conjunto de entrenamiento\n",
    "x_val = x_train[:5000, :]\n",
    "x_train = x_train[5000:, :]\n",
    "y_val = y_train[:5000]\n",
    "y_train = y_train[5000:]\n",
    "\n",
    "# Vectores y_train, y_val e y_test son transformados en matrices categóricas\n",
    "y_train_ = np_utils.to_categorical(y_train)\n",
    "y_val_ = np_utils.to_categorical(y_val)\n",
    "y_test_ = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.1 Definición de modelo a ser pre-entrenado**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, se diseña la red neuronal que será pre-entrenada. Está será una red FF de dos capas ocultas, las que poseerán 1000 neuronas, cada una. La función de activación de las neuronas será sigmoide. La capa de salida estará conformada por 10 neuronas (una por cada dígito), las que serán activadas por medio de la función softmax.\n",
    "\n",
    "Para efectos comparativos, se entrenará la red sin pre-entrenamiento y se determinará el error de clasificación sobre el conjunto de pruebas. Este entrenamiento será llevado cabo a través del método de optimización *adam*, 50 epochs, batches de tamaño 25 y categorical crossentropy como función de pérdida. **Notar que a pesar de que se dió la instrucción de utilizar SGD, esto no ha sido posible, ya que al entrenar el modelo con este método, las tasas de error obtenidas (tanto de entrenamiento como de validación) eran cercanas al 91%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 41s - loss: 0.3219 - acc: 0.9007 - val_loss: 0.1339 - val_acc: 0.9584\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 41s - loss: 0.1259 - acc: 0.9615 - val_loss: 0.1091 - val_acc: 0.9682\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 41s - loss: 0.0790 - acc: 0.9755 - val_loss: 0.0898 - val_acc: 0.9728\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0542 - acc: 0.9827 - val_loss: 0.0759 - val_acc: 0.9776\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0388 - acc: 0.9874 - val_loss: 0.0703 - val_acc: 0.9802\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 43s - loss: 0.0297 - acc: 0.9900 - val_loss: 0.0924 - val_acc: 0.9744\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 43s - loss: 0.0229 - acc: 0.9923 - val_loss: 0.0767 - val_acc: 0.9814\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0174 - acc: 0.9944 - val_loss: 0.0830 - val_acc: 0.9818\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 38s - loss: 0.0150 - acc: 0.9947 - val_loss: 0.0978 - val_acc: 0.9804\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 40s - loss: 0.0122 - acc: 0.9961 - val_loss: 0.0957 - val_acc: 0.9788\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0108 - acc: 0.9964 - val_loss: 0.1062 - val_acc: 0.9790\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0104 - acc: 0.9964 - val_loss: 0.0902 - val_acc: 0.9814\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0068 - acc: 0.9979 - val_loss: 0.0923 - val_acc: 0.9806\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0077 - acc: 0.9976 - val_loss: 0.1051 - val_acc: 0.9826\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0065 - acc: 0.9980 - val_loss: 0.0843 - val_acc: 0.9838\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0070 - acc: 0.9976 - val_loss: 0.1307 - val_acc: 0.9798\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0074 - acc: 0.9977 - val_loss: 0.0943 - val_acc: 0.9828\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0045 - acc: 0.9986 - val_loss: 0.1036 - val_acc: 0.9832\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0048 - acc: 0.9985 - val_loss: 0.1347 - val_acc: 0.9804\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0072 - acc: 0.9980 - val_loss: 0.1110 - val_acc: 0.9822\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 43s - loss: 0.0024 - acc: 0.9993 - val_loss: 0.1002 - val_acc: 0.9830\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0082 - acc: 0.9980 - val_loss: 0.1165 - val_acc: 0.9814\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0032 - acc: 0.9992 - val_loss: 0.1187 - val_acc: 0.9822\n",
      "Epoch 24/50\n",
      "55000/55000 [==============================] - 41s - loss: 0.0056 - acc: 0.9984 - val_loss: 0.1104 - val_acc: 0.9836\n",
      "Epoch 25/50\n",
      "55000/55000 [==============================] - 43s - loss: 0.0049 - acc: 0.9985 - val_loss: 0.1012 - val_acc: 0.9868\n",
      "Epoch 26/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.1070 - val_acc: 0.9842\n",
      "Epoch 27/50\n",
      "55000/55000 [==============================] - 40s - loss: 0.0065 - acc: 0.9983 - val_loss: 0.1042 - val_acc: 0.9854\n",
      "Epoch 28/50\n",
      "55000/55000 [==============================] - 39s - loss: 0.0023 - acc: 0.9994 - val_loss: 0.1043 - val_acc: 0.9856\n",
      "Epoch 29/50\n",
      "55000/55000 [==============================] - 41s - loss: 0.0046 - acc: 0.9986 - val_loss: 0.1114 - val_acc: 0.9846\n",
      "Epoch 30/50\n",
      "55000/55000 [==============================] - 40s - loss: 0.0033 - acc: 0.9991 - val_loss: 0.1125 - val_acc: 0.9836\n",
      "Epoch 31/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0040 - acc: 0.9990 - val_loss: 0.1121 - val_acc: 0.9846\n",
      "Epoch 32/50\n",
      "55000/55000 [==============================] - 40s - loss: 0.0025 - acc: 0.9994 - val_loss: 0.1180 - val_acc: 0.9834\n",
      "Epoch 33/50\n",
      "55000/55000 [==============================] - 40s - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0986 - val_acc: 0.9860\n",
      "Epoch 34/50\n",
      "55000/55000 [==============================] - 41s - loss: 0.0027 - acc: 0.9994 - val_loss: 0.1282 - val_acc: 0.9818\n",
      "Epoch 35/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0044 - acc: 0.9988 - val_loss: 0.1243 - val_acc: 0.9830\n",
      "Epoch 36/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0038 - acc: 0.9990 - val_loss: 0.1219 - val_acc: 0.9836\n",
      "Epoch 37/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0028 - acc: 0.9992 - val_loss: 0.1267 - val_acc: 0.9834\n",
      "Epoch 38/50\n",
      "55000/55000 [==============================] - 40s - loss: 0.0040 - acc: 0.9987 - val_loss: 0.1369 - val_acc: 0.9838\n",
      "Epoch 39/50\n",
      "55000/55000 [==============================] - 41s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.1390 - val_acc: 0.9828\n",
      "Epoch 40/50\n",
      "55000/55000 [==============================] - 41s - loss: 0.0046 - acc: 0.9989 - val_loss: 0.1483 - val_acc: 0.9804\n",
      "Epoch 41/50\n",
      "55000/55000 [==============================] - 41s - loss: 0.0035 - acc: 0.9991 - val_loss: 0.1287 - val_acc: 0.9834\n",
      "Epoch 42/50\n",
      "55000/55000 [==============================] - 41s - loss: 0.0031 - acc: 0.9991 - val_loss: 0.1337 - val_acc: 0.9824\n",
      "Epoch 43/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0029 - acc: 0.9993 - val_loss: 0.1176 - val_acc: 0.9850\n",
      "Epoch 44/50\n",
      "55000/55000 [==============================] - 44s - loss: 0.0024 - acc: 0.9994 - val_loss: 0.1065 - val_acc: 0.9856\n",
      "Epoch 45/50\n",
      "55000/55000 [==============================] - 40s - loss: 0.0034 - acc: 0.9994 - val_loss: 0.1600 - val_acc: 0.9816\n",
      "Epoch 46/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0042 - acc: 0.9988 - val_loss: 0.1249 - val_acc: 0.9836\n",
      "Epoch 47/50\n",
      "55000/55000 [==============================] - 41s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.1401 - val_acc: 0.9830\n",
      "Epoch 48/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0036 - acc: 0.9990 - val_loss: 0.1274 - val_acc: 0.9844\n",
      "Epoch 49/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.1275 - val_acc: 0.9822\n",
      "Epoch 50/50\n",
      "55000/55000 [==============================] - 42s - loss: 0.0019 - acc: 0.9996 - val_loss: 0.1273 - val_acc: 0.9834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f76752a8250>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se define red vacía\n",
    "model = Sequential()\n",
    "# Se agregan capas ocultas\n",
    "model.add(Dense(1000, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dense(1000, activation='sigmoid'))\n",
    "# Se agrega capa de salida\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# Se definen parámetros de entrenamiento\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Se entrena modelo\n",
    "model.fit(x_train, y_train_, epochs=50, batch_size=25, shuffle=True, validation_data=(x_val, y_val_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9760/10000 [============================>.] - ETA: 0s\n",
      "Error de clasificacion sobre conjunto de pruebas:  1.79 %\n"
     ]
    }
   ],
   "source": [
    "# Se determina error de clasificación sobre conjunto de pruebas\n",
    "model_scores = model.evaluate(x_test, y_test_)\n",
    "print '\\nError de clasificacion sobre conjunto de pruebas: ', (1 - model_scores[1]) * 100, '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, el entrenamiento finaliza con función de pérdida 0,0019 y error de clasificación de entrenamiento de un 0,04%. Además, el error de clasificación sobre el conjunto de pruebas es de un 1,79%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.2 Pre-entrenando el modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Con el fin de mejorar el error de clasificación, se pre-entrenará la red diseñada en la sección anterior. Como dicha red posee dos capas ocultas, se utilizarán dos autoencoders, cada uno encargado de pre-entrenar los pesos de una y sólo una de las capas. Para llevar a cabo esta tarea, se implementa la función *pre_training*, la que recibe el nombre de la función de activación a ser utilizada tanto en los autoencoders como en el modelo. Como resultado de este proceso, el modelo de la sección anterior será entrenado de tal manera que los pesos de cada capa oculta serán los mismos que pre-entrenó el autoencoder respectivo.\n",
    "\n",
    "Notar que el modelo en estrenado durante sólo 20 epochs, a diferencia del entrenamiento del modelo de la sección anterior, para el cual se emplearon 50 epochs. Se ha tomado esta medida con el fin de comprobar la eficacia de pre-entrenar los pesos. Tal eficacia debiese manifestarse a través de la obtención de una tasa de error de clasificación cercana similar a la conseguida sin pre-entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre_training permite el pre-entrenamiento de una red FF\n",
    "def pre_training(encoder_activation, x_train, x_val, decoder_activation=None, dAE=''):\n",
    "    \n",
    "    if not decoder_activation:\n",
    "        decoder_activation = encoder_activation\n",
    "    \n",
    "    # Se implementa primer autoencoder, encargado de pre-entrenar primera capa oculta\n",
    "    # Se define input a ser recibido por autoencoder\n",
    "    input_img1 = Input(shape=(784, ))\n",
    "    # Se define capa encoder \n",
    "    encoded1 = Dense(1000, activation=encoder_activation)(input_img1)\n",
    "    # Se define capa decoder\n",
    "    decoded1 = Dense(784, activation=decoder_activation)(encoded1)\n",
    "    # Se define autoencoder\n",
    "    autoencoder1 = Model(input=input_img1, output=decoded1)\n",
    "    # Se define encoder por separado\n",
    "    encoder1 = Model(input_img1, output=encoded1)\n",
    "    # Se definen parámetros de pre-entrenamiento\n",
    "    autoencoder1.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    # Se pre-entrena primer autoencoder\n",
    "    autoencoder1.fit(x_train, x_train, epochs=50, batch_size=25, shuffle=True, verbose=0, validation_data=(x_val, x_val))\n",
    "    encoded_input1 = Input(shape=(1000, ))\n",
    "    # Se guardan autoencoder y encoder en archivos\n",
    "    autoencoder1.save('autoencoder_1_' + encoder_activation + dAE + '.h5')\n",
    "    encoder1.save('encoder_1_' + encoder_activation + dAE + '.h5')\n",
    "    \n",
    "    # Se implementa segundo autoencoder, encargado de pre-entrenar segunda capa oculta\n",
    "    # Primero, se obtienen las salidas generadas por el encoder del primer autoencoder para cada conjunto de datos\n",
    "    x_train_encoded1 = encoder1.predict(x_train)\n",
    "    x_val_encoded1 = encoder1.predict(x_val)\n",
    "    x_test_encoded1 = encoder1.predict(x_test)\n",
    "\n",
    "    # Se define input a ser recibido por autoencoder\n",
    "    input_img2 = Input(shape=(1000, ))\n",
    "    # Se define capa encoder\n",
    "    encoded2 = Dense(1000, activation=encoder_activation)(input_img2)\n",
    "    # Se define capa decoder\n",
    "    decoded2 = Dense(1000, activation=decoder_activation)(encoded2)\n",
    "    # Se define autoencoder\n",
    "    autoencoder2 = Model(input=input_img2, output=decoded2)\n",
    "    # Se define encoder por separado\n",
    "    encoder2 = Model(input=input_img2, output=encoded2)\n",
    "    # Se especifican parámetros de optimización\n",
    "    autoencoder2.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    # Se pre-entrena segundo autoencoder\n",
    "    autoencoder2.fit(x_train_encoded1, x_train_encoded1, epochs=50, batch_size=25, shuffle=True, verbose=0,\n",
    "                 validation_data=(x_val_encoded1, x_val_encoded1))\n",
    "    encoded_input2 = Input(shape=(1000, ))\n",
    "    autoencoder2.save('autoencoder_2_' + encoder_activation + dAE + '.h5')\n",
    "    encoder2.save('encoder_2_' + encoder_activation + dAE + '.h5')\n",
    "\n",
    "    # Finalmente, se entrena modelo a partir de pesos pre-entrenados\n",
    "    model = Sequential()\n",
    "    # Se agrega primera capa oculta\n",
    "    model.add(Dense(1000, activation=encoder_activation, input_shape=(784, )))\n",
    "    # Los pesos de la primera capa oculta son igualados a pesos pre-entrenados por primer autoencoder\n",
    "    model.layers[-1].set_weights(autoencoder1.layers[1].get_weights())\n",
    "    # Se agrega segunda capa oculta\n",
    "    model.add(Dense(1000, activation=decoder_activation))\n",
    "    # Los pesos de la segunda capa oculta son igualados a pesos pre-entrenados por segundo autoencoder\n",
    "    model.layers[-1].set_weights(autoencoder2.layers[1].get_weights())\n",
    "    # Se define capa de salida\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # Se especifican parámetros de optimización\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # Se entrena modeo\n",
    "    model.fit(x_train, y_train_, epochs=20, batch_size=25, shuffle=True, verbose=1 , validation_data=(x_val, y_val_))\n",
    "    # Se guarda modelo entrenado en archivo\n",
    "    model.save('Net-784x1000x1000x10-finetunned_' + encoder_activation + dAE + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, se ejecuta el proceso descrito sobre el modelo de la sección anterior utilizando la función de activación sigmoide tanto en los autoencoders como en las capas ocultas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 41s - loss: 0.2367 - acc: 0.9277 - val_loss: 0.1243 - val_acc: 0.9630\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 41s - loss: 0.0889 - acc: 0.9722 - val_loss: 0.0866 - val_acc: 0.9728\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 41s - loss: 0.0537 - acc: 0.9831 - val_loss: 0.0817 - val_acc: 0.9742\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 41s - loss: 0.0360 - acc: 0.9883 - val_loss: 0.0769 - val_acc: 0.9796\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 41s - loss: 0.0243 - acc: 0.9920 - val_loss: 0.0764 - val_acc: 0.9802\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 41s - loss: 0.0198 - acc: 0.9935 - val_loss: 0.0696 - val_acc: 0.9816\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 41s - loss: 0.0156 - acc: 0.9944 - val_loss: 0.0691 - val_acc: 0.9826\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 42s - loss: 0.0123 - acc: 0.9960 - val_loss: 0.0906 - val_acc: 0.9786\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 42s - loss: 0.0097 - acc: 0.9967 - val_loss: 0.0829 - val_acc: 0.9836\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 43s - loss: 0.0098 - acc: 0.9965 - val_loss: 0.1009 - val_acc: 0.9780\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 39s - loss: 0.0093 - acc: 0.9969 - val_loss: 0.0880 - val_acc: 0.9836\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 39s - loss: 0.0073 - acc: 0.9973 - val_loss: 0.0826 - val_acc: 0.9816\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 39s - loss: 0.0066 - acc: 0.9979 - val_loss: 0.0781 - val_acc: 0.9836\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0061 - acc: 0.9979 - val_loss: 0.0919 - val_acc: 0.9838\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0970 - val_acc: 0.9830\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0072 - acc: 0.9976 - val_loss: 0.1399 - val_acc: 0.9800\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0855 - val_acc: 0.9846\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0050 - acc: 0.9985 - val_loss: 0.0851 - val_acc: 0.9852\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0053 - acc: 0.9983 - val_loss: 0.1052 - val_acc: 0.9842\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0045 - acc: 0.9985 - val_loss: 0.1229 - val_acc: 0.9792\n",
      " 9600/10000 [===========================>..] - ETA: 0s\n",
      "Error de clasificacion sobre conjunto de pruebas:  2.29 %\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pre_training('sigmoid', x_train, x_val)\n",
    "model_sigmoid = load_model('Net-784x1000x1000x10-finetunned_sigmoid.h5')\n",
    "model_sigmoid_scores = model_sigmoid.evaluate(x_test, y_test_)\n",
    "print '\\nError de clasificacion sobre conjunto de pruebas: ', (1 - model_sigmoid_scores[1]) * 100, '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ve entonces que el entrenamiento finaliza con una función de pérdida de 0,0045 y error de clasificación de 0,15%, valores superiores a los obtenidos sin pre-entrenamiento, a pesar de que los valores iniciales tanto de pérdida como de error fueron más bajos en la primera iteración con respecto al modelo original, lo que quiere decir que, en este caso, los pesos se econtraban más cerca de sus valores óptimos al momento de iniciarse el entrenamiento. \n",
    "\n",
    "Se tiene también que el error de clasificación sobre el conjunto de pruebas aumenta a un 2,29%, por lo que los resultados obtenidos no son del todo satisfactorios. Dada esta situación, es interesante estudiar los resultados que se obtienen al utilizar otras funciones de activación (ver sección 3.3.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.3 Pre-entrenamiento vía *denoising autoencoder***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el fin de mejorar el poder de generalización del clasificador, se introduce ruido en las imágenes, repitiendo el proceso ejecutado en la sección anterior, pero pre-entrenando los pesos a partir de las imágenes corruptas. Así, se \"corrompen\" los conjuntos de entrenamiento y de validación tal y como se hizo en el problema 3.2., con el parámetro devst seteado en 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se genera versión corrupta de imágenes de cada conjunto de datos\n",
    "\n",
    "# Se utiliza desviación estándar 0.1\n",
    "devst = 0.1\n",
    "\n",
    "# Se genera versión corrupta de datos de entrenamiento\n",
    "noise_mask_train = devst * np.random.standard_normal(size=x_train.shape)\n",
    "noisy_x_train = x_train + noise_mask_train\n",
    "\n",
    "# Se genera versión corrupta de datos de validación\n",
    "noise_mask_val = devst * np.random.standard_normal(size=x_val.shape)\n",
    "noisy_x_val = x_val + noise_mask_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generados los nuevos conjuntos de datos, se procede a pre-entrenar y posteriormente entrenar el modelo original por medio de dichos conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 41s - loss: 0.3665 - acc: 0.8882 - val_loss: 0.2077 - val_acc: 0.9348\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 41s - loss: 0.1469 - acc: 0.9541 - val_loss: 0.1395 - val_acc: 0.9568\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 41s - loss: 0.0744 - acc: 0.9769 - val_loss: 0.1282 - val_acc: 0.9642\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 41s - loss: 0.0376 - acc: 0.9881 - val_loss: 0.1397 - val_acc: 0.9618\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 43s - loss: 0.0224 - acc: 0.9926 - val_loss: 0.1295 - val_acc: 0.9678\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0161 - acc: 0.9947 - val_loss: 0.1412 - val_acc: 0.9704\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0142 - acc: 0.9954 - val_loss: 0.1220 - val_acc: 0.9750\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0125 - acc: 0.9960 - val_loss: 0.1345 - val_acc: 0.9708\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0086 - acc: 0.9971 - val_loss: 0.1581 - val_acc: 0.9704\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0091 - acc: 0.9969 - val_loss: 0.1428 - val_acc: 0.9720\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0084 - acc: 0.9974 - val_loss: 0.1441 - val_acc: 0.9738\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0069 - acc: 0.9977 - val_loss: 0.1511 - val_acc: 0.9764\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0080 - acc: 0.9977 - val_loss: 0.2006 - val_acc: 0.9648\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0049 - acc: 0.9986 - val_loss: 0.1663 - val_acc: 0.9724\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0080 - acc: 0.9977 - val_loss: 0.1633 - val_acc: 0.9726\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0058 - acc: 0.9983 - val_loss: 0.1781 - val_acc: 0.9694\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0069 - acc: 0.9977 - val_loss: 0.1568 - val_acc: 0.9766\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0061 - acc: 0.9982 - val_loss: 0.1533 - val_acc: 0.9738\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0059 - acc: 0.9983 - val_loss: 0.1882 - val_acc: 0.9722\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0038 - acc: 0.9989 - val_loss: 0.1867 - val_acc: 0.9700\n",
      " 9664/10000 [===========================>..] - ETA: 0s\n",
      "Error de clasificacion sobre conjunto de pruebas:  2.4 %\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pre_training('sigmoid', noisy_x_train, noisy_x_val, dAE='_dAE')\n",
    "model_sigmoid_dAE = load_model('Net-784x1000x1000x10-finetunned_sigmoid_dAE.h5')\n",
    "model_sigmoid_dAE_scores = model_sigmoid_dAE.evaluate(x_test, y_test_)\n",
    "print '\\nError de clasificacion sobre conjunto de pruebas: ', (1 - model_sigmoid_dAE_scores[1]) * 100, '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados muestran que el entrenamiento termina con función de pérdida 0,0038 y error de clasificación de 0,11%, mejorando levemente los resultados de la sección anterior, a pesar de que los valores iniciales de la pérdida y el error fueron más altos en este último caso. A pesar de esto, el error de clasificación sobre el conjunto de pruebas es de un 2,4%, siendo superior al error obtenido en la sección 3.3.2 por un escaso margen. \n",
    "\n",
    "De esta forma, el haber agregado ruido a los datos no ha ofrecido mejoras. Quizás hubiese resultado interesante trabajar con conjuntos de datos formados tanto por imágenes corruptas como imágenes normales.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.4 Pre-entrenamiento con tanh y ReLU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es de interés conocer como varían los resultados de la sección 3.3.2 al modificar la función de activación tanto en el encoder como el decoder del autoencoder construído. Primero, se estudia el efector de utilizar la función tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.2519 - acc: 0.9237 - val_loss: 0.1382 - val_acc: 0.9596\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.1325 - acc: 0.9594 - val_loss: 0.1143 - val_acc: 0.9668\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.1108 - acc: 0.9659 - val_loss: 0.1159 - val_acc: 0.9686\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 39s - loss: 0.0943 - acc: 0.9705 - val_loss: 0.1024 - val_acc: 0.9726\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 39s - loss: 0.0826 - acc: 0.9747 - val_loss: 0.1101 - val_acc: 0.9654\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 39s - loss: 0.0747 - acc: 0.9769 - val_loss: 0.1044 - val_acc: 0.9716\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0712 - acc: 0.9767 - val_loss: 0.1284 - val_acc: 0.9684\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0630 - acc: 0.9791 - val_loss: 0.1229 - val_acc: 0.9686\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0641 - acc: 0.9798 - val_loss: 0.1185 - val_acc: 0.9714\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0573 - acc: 0.9819 - val_loss: 0.1106 - val_acc: 0.9716\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0576 - acc: 0.9822 - val_loss: 0.1257 - val_acc: 0.9702\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0549 - acc: 0.9829 - val_loss: 0.1259 - val_acc: 0.9698\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0516 - acc: 0.9838 - val_loss: 0.1337 - val_acc: 0.9668\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0472 - acc: 0.9847 - val_loss: 0.1247 - val_acc: 0.9694\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0499 - acc: 0.9840 - val_loss: 0.1206 - val_acc: 0.9696\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0474 - acc: 0.9851 - val_loss: 0.1383 - val_acc: 0.9718\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0446 - acc: 0.9860 - val_loss: 0.1176 - val_acc: 0.9740\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0453 - acc: 0.9861 - val_loss: 0.1190 - val_acc: 0.9744\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0480 - acc: 0.9856 - val_loss: 0.1312 - val_acc: 0.9734\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0445 - acc: 0.9863 - val_loss: 0.1261 - val_acc: 0.9726\n",
      " 9888/10000 [============================>.] - ETA: 0s \n",
      "Error de clasificacion sobre conjunto de pruebas:  2.82 %\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pre_training('tanh', x_train, x_val)\n",
    "model_tanh = load_model('Net-784x1000x1000x10-finetunned_tanh.h5')\n",
    "model_tanh_scores = model_tanh.evaluate(x_test, y_test_)\n",
    "print '\\nError de clasificacion sobre conjunto de pruebas: ', (1 - model_tanh_scores[1]) * 100, '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que la utilización de tanh tanto en el encoder como el decoder empeora los resultados en comparación con sigmoide: La función de pérdida y el error de clasificación poseen valores iniciales y finales más altos con tanh que con sigmoide. También, el error de clasificación sobre el conjunto de pruebas aumenta de 1,79% con sigmoide a 2,82% con tanh, por lo que queda en evidencia que no es conveniente trabajar con esta última función para resolver el problema planteado. \n",
    "\n",
    "De forma análoga, se estudia el efecto de utilizar la función ReLU como función de activación. Recordando que previamente se obtuvieron  malos resultados al utilizar ReLU en el decoder (problema 3.1), se usará esta función sólo en el encoder, mientras que en el decoder nuevamente se usará sigmoide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.1843 - acc: 0.9433 - val_loss: 0.1020 - val_acc: 0.9676\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0687 - acc: 0.9785 - val_loss: 0.0774 - val_acc: 0.9780\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 38s - loss: 0.0447 - acc: 0.9852 - val_loss: 0.0861 - val_acc: 0.9780\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0313 - acc: 0.9898 - val_loss: 0.0746 - val_acc: 0.9788\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0247 - acc: 0.9920 - val_loss: 0.0782 - val_acc: 0.9804\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0190 - acc: 0.9939 - val_loss: 0.0871 - val_acc: 0.9794\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0157 - acc: 0.9949 - val_loss: 0.0916 - val_acc: 0.9804\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0136 - acc: 0.9954 - val_loss: 0.0930 - val_acc: 0.9800\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0124 - acc: 0.9959 - val_loss: 0.0932 - val_acc: 0.9840\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0119 - acc: 0.9961 - val_loss: 0.1033 - val_acc: 0.9808\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0098 - acc: 0.9968 - val_loss: 0.0960 - val_acc: 0.9802\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0102 - acc: 0.9965 - val_loss: 0.0946 - val_acc: 0.9816\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0093 - acc: 0.9972 - val_loss: 0.0942 - val_acc: 0.9848\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0073 - acc: 0.9977 - val_loss: 0.1043 - val_acc: 0.9812\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0082 - acc: 0.9973 - val_loss: 0.1027 - val_acc: 0.9848\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0074 - acc: 0.9979 - val_loss: 0.0902 - val_acc: 0.9852\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0074 - acc: 0.9978 - val_loss: 0.1045 - val_acc: 0.9858\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0072 - acc: 0.9978 - val_loss: 0.1113 - val_acc: 0.9824\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0065 - acc: 0.9980 - val_loss: 0.1080 - val_acc: 0.9828\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 40s - loss: 0.0056 - acc: 0.9984 - val_loss: 0.1052 - val_acc: 0.9834\n",
      " 9280/10000 [==========================>...] - ETA: 0s\n",
      "Error de clasificacion sobre conjunto de pruebas:  1.82 %\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pre_training('relu', x_train, x_val, decoder_activation='sigmoid')\n",
    "model_relu = load_model('Net-784x1000x1000x10-finetunned_relu.h5')\n",
    "model_relu_scores = model_relu.evaluate(x_test, y_test_)\n",
    "print '\\nError de clasificacion sobre conjunto de pruebas: ', (1 - model_relu_scores[1]) * 100, '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Los resultados muestran que, inicialmente, la función de pérdida y el error de clasificación en el entrenamiento poseen valores más bajos respecto al caso en que la función de activación es sigmoide. Lamentablemente, los valores finales de estas mismas métricas terminan siendo mayores en este caso. No obstante, el error de clasificación sobre el conjunto de pruebas resulta ser del 1,82%, bastante cercano al 1,79% que se obtiene con sigmoide.\n",
    "\n",
    "De esta manera, el pre-entrenamiento del modelo original a partir de un autoencoder que utiliza la función ReLU en el encoder y sigmoide en el decoder logra obtener los mejores resultados en términos del error de clasificación sobre el conjunto de pruebas respecto a todos los pre-entrenamientos estudiados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
